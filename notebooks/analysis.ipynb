{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9088f5b4",
   "metadata": {},
   "source": [
    "Overall the biggest difference we noticed was within the training of pacman using PPO vs A2C. A2C would process steps faster, generating outputs quicker than PPO, however the training process within A2C did not succeed overall. While training to relatively same timesteps, we noticed that PPO takes a significantly longer time, as generated by the graph below. Pacmans PPO training allowed us to analyze the DRL Agent actually making progress over time, learning new tactics based on the reward system given to it (Explorer vs Survival), while in A2C, training overtime would have pacman hit a barrier and give up.\n",
    "\n",
    "PPO is very volatile in the beginning. I find that if it doesn't find a good strategy early, it'll stall, and you need to restart. I'm sure using some hyperparameters would help, but I'd rather not have 2 unknowns in my code. \n",
    "A2C seems to be only useful for low-level games like Flappy Bird or Snake (even Snake is pushing it to be honest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb064c",
   "metadata": {},
   "source": [
    "![Pacman Explorer PPO VS A2C](/TopicFixed/notebooks/plots/pacman_explorer_ppo_vs_a2c_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee6609",
   "metadata": {},
   "source": [
    "The length of time it took for each process is displayed below, we can note that PPO takes the longest overall to achieve the same amount of steps as A2C. We also note the significant increases in length over time for PPO, while A2C has a low length of each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000c842",
   "metadata": {},
   "source": [
    "![Pacman overview length](/TopicFixed/notebooks/plots/pacman_overview_length.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb3d6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Now analyzing the overview reward system, we see that Pacman has a significant increase of rewards within the PPO methodology, while A2C remains stable. This is due to the lack of learning that A2C does while training over time(steps) within pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086a8a6",
   "metadata": {},
   "source": [
    "![Pacman Reward Overview](/TopicFixed/notebooks/plots/pacman_overview_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a3a95",
   "metadata": {},
   "source": [
    "Finally for Pacman survivor, we can analyze the survivor PPO vs A2C reward system. We note that over time A2C does not have any significance in increase and remains relatively stable throughout the time period, displaying that even with a new reward system, Pacman is still unable to achieve progress, While PPO shows a strong positive linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743674b",
   "metadata": {},
   "source": [
    "![Pacman Survival Reward Overview](/TopicFixed/notebooks/plots/pacman_survivor_ppo_vs_a2c_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf15309",
   "metadata": {},
   "source": [
    "When analyzing Snake we noted that both A2C and PPO were able to train well. Even though A2C wasnt able to adapt to the training process in pacman, it did extremely well within Snake, adapting and overcoming efficiently. A2C did train MUCH faster, most easily seen in the Survival for snake, where it only took half an hour to reach 3M steps (All were trained to 3M to standardize what we could find (along with time constraints)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda2438",
   "metadata": {},
   "source": [
    "![Snake Explorer PPO VS A2C](/TopicFixed/notebooks/plots/snake_explorer_ppo_vs_a2c_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0691e",
   "metadata": {},
   "source": [
    "Snake was pretty easy to learn from PPO, and once we added the observation space, it helped to see where it was, plus the exact distances to the apples.\n",
    "\n",
    "Giving Snake a bonus for getting towards the apples, but also applying a punishment for walking away, so it wouldn't \"orbit\" the apples, was really important and a massive early bug that had to be perfected. Pacman, on the other hand, learned early to stall in the corner, but to stop that, \n",
    "I just smashed him over the head with a brick if he stood still for 10~ steps and didn't collect any new points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd87a1e",
   "metadata": {},
   "source": [
    "Moving forward we can analyze the Snake Survivor method, witnessing the rewards going down overtime. The reason for this is due to an oversight within the csv configuration, reducing the rewards overtime as snake progresses for a longer period of time. This wrapper focused mainly on length of time of survival, ignoring floating in one setting or repeating a movement for survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0fcdd",
   "metadata": {},
   "source": [
    "![Snake Survivor Rewards](/TopicFixed/notebooks/plots/snake_survivor_ppo_vs_a2c_reward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab27e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "The personas were very different, and the second one was playing the game a different way; it's a bit harder to train to play the actual game versus just living normally. You didn't have to play perfectly or even relatively well, just not die, so it was a lot easier and faster to train the \n",
    "survivor class versus the normal one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0ad96",
   "metadata": {},
   "source": [
    "Main Tasks:\n",
    "1) We have all our \"main\" code under src, the training, testing and evaluation are all written there as well as our common code between all games (could be used for any of them, so common wrappers and the autosaves and the metrics being collected, at least the general structure)\n",
    "Under Envs is where you're going to find the game being built and all the game-specific wrappers.\n",
    "Logs is where all of our game data will be found, checkpoints, and the CSVs for specific game data, as well as the len and rew means. \n",
    "Notebooks are where we have our plotting script (could be moved, but then we would have to update a lot of code, and I didn't want to do that with so little time remaining). Under config is where you'll find all our YAML files (which I still don't like). It gets used by main, it looks for all of our algos, apps and rewards.\n",
    "2) We trained on PPO and A2C already talked about above \n",
    "3) Talked about above, but we used Survivor and Explorer for the two personas \n",
    "4) Metrics THE WORST THING TO DO ON EMULATORS do **NOT**use emulators for this stuff if you can avoid it, but we got the data needed and then some extra stuff I thought would be cool to learn \n",
    "5) Pacman and Snake, you can VERY easily add games into this just by adding to the config, making a wrapper, then adding it to the factory file."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
